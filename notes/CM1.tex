\chapter{Reminders : Asymptotic notation}

\colorbox{green}{TEMPORARY - CM1}

Let 
$ \left\{
\begin{array}{ll}
f :& \mathbb{N} \rightarrow \mathbb{R}^+\\
g :& \mathbb{N} \rightarrow \mathbb{R}^+
\end{array} \right.$ 

\begin{itemize}
\renewcommand{\labelitemi}{$\bullet$}
	\item  $f \in \mathcal{O}(g)$, often written $f(n) = \mathcal{O}(g(n))$, means: $\exists n_0, c : \forall n \geq n_0, f(n) \leq c \cdot g(n)$.
	\item 	
	$f \in \Theta(g)$ $\begin{array}{l} 
	   \Leftrightarrow f \in \mathcal{O}(g)  \text{ and } g \in \mathcal{O}(f) \\
	\Leftrightarrow  \exists n_0, c_1, c_2 : \forall n\geq n_0, c_1 \cdot g(n)\leq f(n) \leq c_2 \cdot g(n) 
	\end{array}$	
	($f$ and $g$ are equivalent in complexity)
\item $f \in o(g) \Leftrightarrow f \in \mathcal{O}(g)$ but $f \notin \Theta(g)$
\item  $f \in \Omega(g) \Leftrightarrow g \in \mathcal{O}(f)$
\item $f \in \omega(g) \Leftrightarrow g \in o(f)$
\end{itemize}

\vspace{0.5cm}
\begin{example}
\begin{leftbar}
\begin{align*} 
	   n^2 & = \mathcal{O}(n^2) &\\
		 & = o(n^3) &\\
		 & = \Theta \left(\frac{n (n-1)}{2}\right) &\\
		& = \Omega (n \log (n))& 
	\end{align*}
	\end{leftbar}
\end{example}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Sorting Algorithms}

We will start with illustrating some ideas on sorting algorithms. The sorting problem may be defined by its input and output. Hence our goal is to find some algorithms such that: 


\begin{lstlisting}[label={list:c1:inout},caption=Inputs and outputs of sorting algorithms]
INPUT  : array T = (T(1), T(2), ..., T(n)) of n numbers
OUTPUT : array T sorted
\end{lstlisting}


\section{Selection Sort}

First, we consider a naÃ¯ve algorithm: \textbf{Selection Sort}. This first algorithm can be formulated as follows:

\begin{lstlisting}[label={list:c1:selection},caption=Pseudo-code of the selection sort algorithm]
for i from 1 to n - 1
    select the smallest element in the sub array from index i to n
    swap it with element at index i
\end{lstlisting}


What about the time complexity of this algorithm?
\begin{align*}
\text{Time } &= \Theta(n) + \Theta(n-1) + \ldots + \Theta(1) \\
& = \Theta(n^2)
\end{align*}

\begin{remark}
The complexity of this algorithm is $\Theta(n^2)$ for each instance. We have Worst case $=$ Best case $=$ Average case time complexity $= \Theta(n^2)$.
\end{remark}

\section{Insertion Sort}

\begin{lstlisting}[label={list:c1:insertion},caption=Pseudo-code of the insertion sort algorithm]
for i from 2 to n % loop invarient : T(1) ... T(i-1) are sorted
    shift T(i) to the left by successive swap until well placed
\end{lstlisting}

\vspace{0.5cm}
\begin{example}
\begin{leftbar}
We can consider a small exemple, the first four elements of the array being sorted:\\
\tikzstyle{block} = [rectangle, draw, 
    text width=0.8cm, text centered]
\begin{tikzpicture}[node distance = 1cm, auto]
    % Place nodes
    \node [block] (one) {1};
    \node [block, right of=one] (two) {3};
    \node [block, right of=two] (three) {5};
    \node [block, right of=three] (four) {9};
    \node [block, right of=four] (five) {\textcolor[rgb]{0,0,1}{4}};
    \node [block, right of=five] (six) {8};
    \node [block, right of=six] (seven) {0};
		\node[rectangle, below of=five] (ind) {i=5};
    % Draw edges
		\draw[->,distance =8pt, thick,draw=blue] (five.north)to[bend right](four.north);
		\draw[->,distance =8pt,thick, draw=blue] (four.north)to[bend right](three.north);
		\draw[->,thick] (ind.north)to(five.south);
\end{tikzpicture}
\end{leftbar}
\end{example}

Regarding the time complexity, we have: 
\begin{itemize}
\renewcommand{\labelitemi}{$\bullet$}
	\item in worst case, at every step we swap until the beginning of the array: worst-case complexity $= \Theta(1)+\ldots + \Theta(n) = \Theta(n^2)$. This case corresponds to an instance sorted in decreasing order. 
	\item Best-case corresponds to an array already sorted: $\Theta(n)$
	\item Average-case: instances come with uniform probability distribution over all possible orders ($=n!$ orders). Then:
	\begin{flalign*}
		\mathbb{E}[Time] & = \mathbb{E}[\sum_{i=2}^n \Theta(t_i)] \hspace{1cm} \text{  With $t_i$ the number of swaps needed for $T(i)$.}& \\
		& = \Theta \left( \sum_{i=2}^n \mathbb{E}[t_i]\right)  \hspace{1cm} \text{ By linearity of $\mathbb{E}$.}&\\
		& = \Theta \left( \sum_{i=2}^n \frac{i}{2}\right) & \\
		& = \Theta(n^2)
	\end{flalign*}
\end{itemize}

In \emph{some} applications, the relevant probability distribution is not uniform but biased towards almost-sorted instances.   \newline 
$\Rightarrow$ Average-case can be $o(n^2)$. 

\section{Quick Sort}
This third algorithm is based on the idea of "Divide-and-Conquer" which solves the sorting problem in the following manner:

\begin{itemize}
\renewcommand{\labelitemi}{$\bullet$}
	\item Split into small sorting problems.
	\item Recombine the outputs, which gives the sorted instance.
\end{itemize}

In particular, in this case:

\begin{lstlisting}[label={list:c1:quick},caption=Pseudo-code of the quick sort algorithm]
pivot T(1)
T_low = [T(i) : T(i) < pivot && i > 1]
T_high = [T(i) : T(i) >= pivot && i > 1]
quicksort T_low
quicksort T_high
T = [T_low pivot T_high] (The pivot being in $T_{low}$)
\end{lstlisting}

About the complexity of this algorithm:
\begin{itemize}
\renewcommand{\labelitemi}{$\bullet$}
\item Worst case: when instance is already sorted ! \newline
In this case: $\left.
	\begin{array}{ll}
       T_{low} & \text{= 1 entry}.\\
       T_{high} &\text{= $n-1$ entries}.
			\end{array}
  \right]$ $\Rightarrow$ maximally unbalanced subproblems. Time $= \Theta(n^2)$ 
	
	Indeed: construct $T_{low}/ T_{right} = \Theta(n)$, hence we have the worst-case recurrence equation: 
	\begin{align*}
t_n & = t_1 + t_{n-1}+ \Theta(n)\\
 & = t_1 + \left( t_1 + t_{n-2} + \Theta(n-1)\right)+ \Theta(n)\\
& = t_1 + \left( t_1 + (t_1 + t_{n-3}+ \Theta(n-2)) + \Theta(n-1)\right)+ \Theta(n)\\
& = \ldots \\
& = n \cdot t_1 + \Theta(n^2) = \Theta(n^2)
\end{align*}

\item Average-case complexity: 
\begin{align}
t_n & = t_{\frac{n}{2}}+ t_{\frac{n}{2}}+ \Theta(n) \label{eq:quickSort_average} \\
& = 2 \cdot t_{\frac{n}{2}} + \Theta(n)  \notag \\
& = 2 \left( 2 \cdot t_{\frac{n}{4}} + \Theta(\frac{n}{2})\right) + \Theta(n)  \notag \\
& =  2 \left( 2 \cdot \left(2 \cdot t_{\frac{n}{8}} + \Theta(\frac{n}{4})\right) + \Theta(\frac{n}{2})\right) + \Theta(n)  \notag \\
& = \ldots  \notag \\
& = 2^{\log_2(n)} \cdot t_1 + \Theta(n \log_2(n))  \notag \\
& = \Theta(n \log_2(n)) \notag
\end{align}
\end{itemize}

\begin{remark}
We will denote $\log_2$ by $\log$, since $\log_a(n)\in \Theta(\log_b(n))$, $\forall a,b >1$. 
\end{remark}

\begin{remark}
In $t_n = t_{\frac{n}{2}}+ t_{\frac{n}{2}} + \Theta(n)$,
\begin{align*}
\mathbb{E}[t_n] & = \mathbb{E}[t_{|T_{low}|}]+ \mathbb{E}[t_{|T_{high}|}] + \Theta(n) \text{ would be more correct.} & \\
& \cong t_{\mathbb{E}[|T_{low}|]} + t_{\mathbb{E}[|T_{high}|]}  + \Theta(n) \text{ : in fact OK.}
\end{align*}
\end{remark}

Now, what if you don't know that your instances are uniformly distributed? This leads to the next algorithm.

\section{Randomized Quick Sort}
Here, the first step of the algorithm is to shuffle entries randomly. It is equivalent to picking a random entry as pivot, instead of $T(1)$.

Now on any instance, Randomized Quick Sort runs in expected time $\Theta(n \log(n))$ $\Rightarrow$ Worst-case expected time of this random algorithm is also $\Theta(n \log(n))$. 

One advantage of randomization: Robin Hood effect, i.e. you take from rich instances and give to the poor. Now, everybody can be unlucky from time to time but lucky most of the time. 

Can we reach worst-case complexity $\mathcal{O}(n \log(n))$ deterministically? 
\newline The answer is yes, as we will see in the next section (Merge Sort). 

\section{Merge Sort}

This algorithm is also based on Divide-and-Conquer approach. 

\begin{lstlisting}[label={list:c1:merge},caption=Pseudo-code of the merge sort algorithm]
T_left = [T(i) : i <= n/2]
T_right = [T(i) : i > n/2]
mergesort T_left
mergesort T_right
T = merge(T_left,T_right) % Theta(n) operations
\end{lstlisting}

Time of merge is $\Theta(n)$, so we have the recurrence equation: 
\begin{align*}
t_n & = 2 \cdot t_{\lceil \frac{n}{2}\rceil}+ \Theta(n) \\
& = \Theta(n \log(n))  \text{ (see equation~\eqref{eq:quickSort_average})}
\end{align*}

Which one to use?
\begin{itemize}
\renewcommand{\labelitemi}{$\bullet$}
	\item Random Quick Sort:
	 \begin{itemize}
		\item $\ominus$ You need to generate randomness: not so easy for a computer!
		\item $\ominus$ You can be unlucky.	
		\item $\oplus$ Hidden constants are smaller if implemented efficiently. 
		\item $\oplus$ can be implemented "in place": on $T$ itself $\rightarrow$ no need for extra memory for intermediate variables.
	\end{itemize} 
	
\end{itemize}